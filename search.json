[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome to my blog",
    "section": "",
    "text": "This is my first post in a Quarto blog. Welcome!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My blog",
    "section": "",
    "text": "Enefit - Predict Energy Behavior of Prosumers, part 1\n\n\n\n\n\n\ntechnical\n\n\n\n\n\n\n\n\n\nJun 4, 2024\n\n\nSameer Sant\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to my blog\n\n\n\n\n\n\n\n\n\n\n\nJun 3, 2024\n\n\nSameer Sant\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/enefit-part-1/enefit-1.html",
    "href": "posts/enefit-part-1/enefit-1.html",
    "title": "Enefit - Predict Energy Behavior of Prosumers, part 1",
    "section": "",
    "text": "I completed part 1 of Jeremy Howard’s course Practical Deep Learning for Coders. To practice what I learned about modeling tabular data, I competed in a recent time series forecasting competition on Kaggle.\nEnefit, one of the largest energy companies in the Baltic region, challenged competitors to create an energy prediction model of Estonian clients that have installed solar panels. Because these “prosumers” both produce and consume energy, they disproportionately create energy imbalance, where expected consumption doesn’t match actual consumption. For Enefit, the result is inefficiency, higher operational costs, and grid instability. Since prosumers are a rapidly increasing share of energy clients, reliable energy predictions are critical.\nSpecifically, our prediction target is the amount produced or consumed by a prosumer given recent records of actual targets, installed capacity, weather data, and relevant energy prices. A standard modeling process would entail an in-depth exploratory analysis followed by building a model. Instead, Jeremy recommends to:\nIn this post, I’ll walk through these steps for my solution. But first, let’s define a validation set."
  },
  {
    "objectID": "posts/enefit-part-1/enefit-1.html#defining-a-validation-set",
    "href": "posts/enefit-part-1/enefit-1.html#defining-a-validation-set",
    "title": "Enefit - Predict Energy Behavior of Prosumers, part 1",
    "section": "1 Defining a validation set",
    "text": "1 Defining a validation set\n\n\nShow code\n! [ -e /content ] && pip install -Uqq fastbook kaggle waterfallcharts treeinterpreter dtreeviz holidays\nimport fastbook\nfastbook.setup_book()\n\n\n\n\nShow code\nfrom fastbook import *\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\nfrom fastai.tabular.all import *\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom dtreeviz.trees import *\nfrom IPython.display import Image, display_svg, SVG\nimport holidays\nimport gc\n\npd.options.display.max_rows = 20\npd.options.display.max_columns = 10\n\nimport warnings\nwarnings.simplefilter('ignore', FutureWarning)\nwarnings.simplefilter('ignore', RuntimeWarning)\nwarnings.simplefilter('ignore', UserWarning)\n\n\nThe released data covers September 1, 2021 to May 29, 2023. During the evaluation period, the models are re-scored 3 times as new data is collected: over February 2024, March 2024, and finally April 2024.\nAn effective validation set should resemble the test data, which is what the model is ultimately scored on. Since our goal is to forecast, the test data will be more recent than the released data. It’s also given that new prosumers can appear or disappear each successive scoring period. Therefore, not only must the validation data be more recent than the training data, it must also include prosumers that are not in the training data.\nFor my final solution, I performed 3-fold cross validation using progressively larger training sets and 1 month validation sets that aligned with the scoring periods: February 2023, March 2023, and April 2023. Each round, I dropped a random set of prosumers from the training set among those common to training and validation just after the split, which made the validation ones new from the model’s point of view. In subsequent rounds, I included the validation month from the previous round in the training set. I then averaged the errors over the three rounds to get a cross validation score. I held out May 2023 as my test set. This is summarized below:\n\n\n\nRound\nTrain\nValid\n\n\n\n\n1\nSept 2021 - Jan 2023\nFeb 2023\n\n\n2\nSept 2021 - Feb 2023\nMar 2023\n\n\n3\nSept 2021 - Mar 2023\nApr 2023\n\n\nTest\nSept 2021 - Apr 2023\nMay 2023\n\n\n\nI used Round 1’s scheme during my development process. Let’s look at the data available at forecast time next."
  },
  {
    "objectID": "posts/enefit-part-1/enefit-1.html#understanding-forecasts",
    "href": "posts/enefit-part-1/enefit-1.html#understanding-forecasts",
    "title": "Enefit - Predict Energy Behavior of Prosumers, part 1",
    "section": "2 Understanding forecasts",
    "text": "2 Understanding forecasts\n\n\nHide code\nURLs.LOCAL_PATH = Path('/notebooks/kaggle/enefit')\npath = URLs.LOCAL_PATH/'predict-energy-behavior-of-prosumers'\n\n# prosumer details\ntest_df = pd.read_csv(path/'example_test_files'/'test.csv', low_memory=False)\nrevealed_targets_df = pd.read_csv(path/'example_test_files'/'revealed_targets.csv', low_memory=False)\nt_client_df = pd.read_csv(path/'example_test_files'/'client.csv', low_memory=False)\n\n# weather tables\nt_forecast_df = pd.read_csv(path/'example_test_files'/'forecast_weather.csv', low_memory=False)\nt_historical_df = pd.read_csv(path/'example_test_files'/'historical_weather.csv', low_memory=False)\n\n# relevant energy prices\nt_electricity_df = pd.read_csv(path/'example_test_files'/'electricity_prices.csv', low_memory=False)\nt_gas_df = pd.read_csv(path/'example_test_files'/'gas_prices.csv', low_memory=False)\n\n# location\nweather_station_to_county_mapping_df = pd.read_csv(path/'weather_station_to_county_mapping.csv', low_memory=False)\n\n\n\n\nHide code\nt_dfs = [test_df, revealed_targets_df, t_client_df, t_forecast_df, t_historical_df, t_electricity_df, t_gas_df, weather_station_to_county_mapping_df]\nt_df_names = ['test', 'revealed_targets', 'client', 'forecast_weather', 'historical_weather', 'electricity_prices', 'gas_prices', 'weather_station_to_county_mapping']\n\n\nBelow is an overview of the columns containing prosumer details:\n\n\nHide code\n{k:v.columns for (k,v) in zip(t_df_names[:3], t_dfs[:3])}\n\n\n{'test': Index(['county', 'is_business', 'product_type', 'is_consumption',\n        'prediction_datetime', 'data_block_id', 'row_id', 'prediction_unit_id',\n        'currently_scored'],\n       dtype='object'),\n 'revealed_targets': Index(['county', 'is_business', 'product_type', 'target', 'is_consumption',\n        'datetime', 'data_block_id', 'row_id', 'prediction_unit_id'],\n       dtype='object'),\n 'client': Index(['product_type', 'county', 'eic_count', 'installed_capacity',\n        'is_business', 'date', 'data_block_id'],\n       dtype='object')}\n\n\nThe key fields in test are:\n\ncounty: an ID for the county\nis_business: a boolean for whether the prosumer is a business\nproduct_type: an ID for the energy contract type, which maps as {0: \"Combined\", 1: \"Fixed\", 2: \"General service\", 3: \"Spot\"}\nis_consumption: a boolean for whether the prediction is for consumption or production\nprediction_datetime: in Estonian time in EET (UTC+2) EEST (UTC+3), the start of the 1-hour period for which the target is to be forecast\nprediction_unit_id: an ID for the prosumer “segment”, i.e. the combination of county, is_business, and product_type, that Enefit uses to define each prosumer.\ndata_block_id: an ID that indicates when the data is available.\n\nThis competition uses a time series API that ensures the model sees only data that would actually be available for each forecast. According to the data dictionary, the data_block_id column is present in all tables and indicates the available data; rows with the same data_block_id participate in the same forecast.\nThe API gets these rows across all tables and serves them as a group (a data block). Once a forecast is made, the process is repeated for the next data_block_id. We are also allowed to store incoming data for the current forecast and use it in future forecasts as lagged features.\nAll forecasts are made at 11am each day for each hour (12am-11pm) the next day. For example, let’s say today is 2023-05-30. At 11am today, the API gets rows from:\n\ntest: tomorrow’s datetimes (2023-05-31 00:00:00 to 2023-05-31 23:00:00). This is the current forecast horizon.\n\n\n\nHide code\ntest_df[test_df['data_block_id'] == 637].head(3)\n\n\n\n\n\n\n\n\n\n\ncounty\nis_business\nproduct_type\nis_consumption\nprediction_datetime\ndata_block_id\nrow_id\nprediction_unit_id\ncurrently_scored\n\n\n\n\n9360\n0\n0\n1\n0\n2023-05-31 00:00:00\n637\n2015232\n0\nFalse\n\n\n9361\n0\n0\n1\n1\n2023-05-31 00:00:00\n637\n2015233\n0\nFalse\n\n\n9362\n0\n0\n2\n0\n2023-05-31 00:00:00\n637\n2015234\n1\nFalse\n\n\n\n\n\n\n\n\n\nrevealed_targets: yesterday’s actual targets (2023-05-29 00:00:00 to 2023-05-29 23:00:00). These datetimes lag 2 days behind the forecast horizon.\n\n\n\nHide code\nrevealed_targets_df[revealed_targets_df['data_block_id'] == 637].head(3)\n\n\n\n\n\n\n\n\n\n\ncounty\nis_business\nproduct_type\ntarget\nis_consumption\ndatetime\ndata_block_id\nrow_id\nprediction_unit_id\n\n\n\n\n9456\n0\n0\n1\n2.073\n0\n2023-05-29 00:00:00\n637\n2008992\n0\n\n\n9457\n0\n0\n1\n503.735\n1\n2023-05-29 00:00:00\n637\n2008993\n0\n\n\n9458\n0\n0\n2\n0.000\n0\n2023-05-29 00:00:00\n637\n2008994\n1\n\n\n\n\n\n\n\n\n\nclient: yesterday’s records of installed capacity (2023-05-29). These dates lag 2 days behind the forecast horizon.\n\n\n\nHide code\nt_client_df[t_client_df['data_block_id'] == 637].head(3)\n\n\n\n\n\n\n\n\n\n\nproduct_type\ncounty\neic_count\ninstalled_capacity\nis_business\ndate\ndata_block_id\n\n\n\n\n197\n1\n0\n508\n4964.215\n0\n2023-05-29\n637\n\n\n198\n2\n0\n10\n31.000\n0\n2023-05-29\n637\n\n\n199\n3\n0\n1515\n15963.060\n0\n2023-05-29\n637\n\n\n\n\n\n\n\n\n\nforecast_weather: forecasted weather applicable from 3am today to 2am the day after tomorrow (2023-05-30 3:00:00 to 2023-06-01 2:00:00) at the given coordinates. These datetimes include the current forecast horizon.\n\n\n\nHide code\nt_forecast_df[t_forecast_df['data_block_id'] == 637].head(3)\n\n\n\n\n\n\n\n\n\n\nlatitude\nlongitude\norigin_datetime\nhours_ahead\ntemperature\n...\nforecast_datetime\ndirect_solar_radiation\nsurface_solar_radiation_downwards\nsnowfall\ntotal_precipitation\n\n\n\n\n16128\n57.6\n21.7\n2023-05-30 02:00:00\n1\n10.190088\n...\n2023-05-30 03:00:00\n0.0\n0.0\n0.0\n0.0\n\n\n16129\n57.6\n22.2\n2023-05-30 02:00:00\n1\n6.493555\n...\n2023-05-30 03:00:00\n0.0\n0.0\n0.0\n0.0\n\n\n16130\n57.6\n22.7\n2023-05-30 02:00:00\n1\n10.304346\n...\n2023-05-30 03:00:00\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n3 rows × 18 columns\n\n\n\n\n\nhistorical_weather: historical weather from 11am yesterday to 1 hour ago (2023-05-29 11:00:00 to 2023-05-30 10:00:00) at the given weather station coordinates. These datetimes lag at least 1 day behind the forecast horizon.\n\n\n\nHide code\nt_historical_df[t_historical_df['data_block_id'] == 637].head(3)\n\n\n\n\n\n\n\n\n\n\ndatetime\ntemperature\ndewpoint\nrain\nsnowfall\n...\ndirect_solar_radiation\ndiffuse_radiation\nlatitude\nlongitude\ndata_block_id\n\n\n\n\n8064\n2023-05-29 11:00:00\n15.0\n10.0\n0.0\n0.0\n...\n489.0\n129.0\n57.6\n21.7\n637.0\n\n\n8065\n2023-05-29 11:00:00\n14.7\n9.4\n0.0\n0.0\n...\n456.0\n161.0\n57.6\n22.2\n637.0\n\n\n8066\n2023-05-29 11:00:00\n14.3\n9.8\n0.0\n0.0\n...\n408.0\n197.0\n57.6\n22.7\n637.0\n\n\n\n\n3 rows × 18 columns\n\n\n\n\n\nelectricity_prices: day-ahead electricity prices obtained yesterday, but applicable 12am-11pm today (2023-05-30 00:00:00 to 2023-05-30 23:00:00). These datetimes lag 1 day behind the forecast horizon.\n\n\n\nHide code\nt_electricity_df[t_electricity_df['data_block_id'] == 637].head(3)\n\n\n\n\n\n\n\n\n\n\nforecast_date\neuros_per_mwh\norigin_date\ndata_block_id\n\n\n\n\n72\n2023-05-30 00:00:00\n8.57\n2023-05-29 00:00:00\n637\n\n\n73\n2023-05-30 01:00:00\n8.06\n2023-05-29 01:00:00\n637\n\n\n74\n2023-05-30 02:00:00\n9.04\n2023-05-29 02:00:00\n637\n\n\n\n\n\n\n\n\n\ngas_prices: day-ahead gas prices obtained yesterday, but applicable today (2023-05-30). These dates lag 1 day behind the forecast horizon.\n\n\n\nHide code\nt_gas_df[t_gas_df['data_block_id'] == 637].head(3)\n\n\n\n\n\n\n\n\n\n\nforecast_date\nlowest_price_per_mwh\nhighest_price_per_mwh\norigin_date\ndata_block_id\n\n\n\n\n3\n2023-05-30\n29.0\n34.0\n2023-05-29\n637\n\n\n\n\n\n\n\n\nNext, we’ll see how to merge this data."
  },
  {
    "objectID": "posts/enefit-part-1/enefit-1.html#merging-the-tables",
    "href": "posts/enefit-part-1/enefit-1.html#merging-the-tables",
    "title": "Enefit - Predict Energy Behavior of Prosumers, part 1",
    "section": "3 Merging the tables",
    "text": "3 Merging the tables\nSince test contains the segments and the datetimes for which we are making forecasts, we want to keep all of the records in test and add matching records from the other tables to it. A left join will achieve this.\nHere are the date ranges in our example data block:\n\n\nHide code\ndate_col_names = ['prediction_datetime', 'datetime', 'date', 'forecast_datetime', 'datetime', 'forecast_date', 'forecast_date']\ndate_cols = [df.loc[df['data_block_id'] == 637, date_col_names[i]] for i,df in enumerate(t_dfs[:-1])]\ndate_ranges = [(t_df_names[i], date_cols[i].min(), date_cols[i].max()) for i,df in enumerate(t_dfs[:-1])]\ndate_ranges\n\n\n[('test', '2023-05-31 00:00:00', '2023-05-31 23:00:00'),\n ('revealed_targets', '2023-05-29 00:00:00', '2023-05-29 23:00:00'),\n ('client', '2023-05-29', '2023-05-29'),\n ('forecast_weather', '2023-05-30 03:00:00', '2023-06-01 02:00:00'),\n ('historical_weather', '2023-05-29 11:00:00', '2023-05-30 10:00:00'),\n ('electricity_prices', '2023-05-30 00:00:00', '2023-05-30 23:00:00'),\n ('gas_prices', '2023-05-30', '2023-05-30')]\n\n\nWe can visualize them in the figure below. To align revealed_targets, client, historical_weather, electricity_prices, and gas_prices, we simply add back how much the respective datetimes lag those in test using a timedelta, effectively moving them up as lagged features. Note that we’ll have historical_weather from 12am-10am only as a result, shown in yellow.\nforecast_weather includes all of test’s datetimes, so we’ll just filter them by the hours_ahead column (meaning hours ahead of the forecast origin), i.e. hours ahead &gt;= 22 and &lt;= 45, shown in green.\n\nWith the datetimes aligned, all but the weather tables are ready to be merged.\nBoth weather tables include latitude/longitude coordinates, but not a county ID as in test. Fortunately, we’re provided a weather_station_to_county_mapping table to do just that:\n\n\nHide code\nweather_station_to_county_mapping_df[~weather_station_to_county_mapping_df['county'].isna()].head()\n\n\n\n\n\n\n\n\n\n\ncounty_name\nlongitude\nlatitude\ncounty\n\n\n\n\n10\nSaaremaa\n22.2\n58.2\n10.0\n\n\n11\nSaaremaa\n22.2\n58.5\n10.0\n\n\n19\nSaaremaa\n22.7\n58.5\n10.0\n\n\n20\nHiiumaa\n22.7\n58.8\n1.0\n\n\n27\nSaaremaa\n23.2\n58.5\n10.0\n\n\n\n\n\n\n\n\nWe’ll left join weather_station_to_county_mapping onto the weather tables on latitude/longitude to add a county ID.\nNote that there are multiple weather readings for each county ID. Here are a few examples from the forecast_weather table:\n\n\nHide code\nt_forecast_df = t_forecast_df.merge(weather_station_to_county_mapping_df, on=['latitude', 'longitude'], how='left')\nt_forecast_df[~t_forecast_df['county'].isna()][['county', 'forecast_datetime']].head()\n\n\n\n\n\n\n\n\n\n\ncounty\nforecast_datetime\n\n\n\n\n10\n15.0\n2023-05-27 03:00:00\n\n\n11\n15.0\n2023-05-27 03:00:00\n\n\n23\n13.0\n2023-05-27 03:00:00\n\n\n24\n15.0\n2023-05-27 03:00:00\n\n\n25\n15.0\n2023-05-27 03:00:00\n\n\n\n\n\n\n\n\nAnd from the historical_weather table:\n\n\nHide code\nt_historical_df = t_historical_df.merge(weather_station_to_county_mapping_df, on=['latitude', 'longitude'], how='left')\nt_historical_df[~t_historical_df['county'].isna()][['county', 'datetime']].head()\n\n\n\n\n\n\n\n\n\n\ncounty\ndatetime\n\n\n\n\n10\n15.0\n2023-05-26 11:00:00\n\n\n11\n15.0\n2023-05-26 11:00:00\n\n\n23\n13.0\n2023-05-26 11:00:00\n\n\n24\n15.0\n2023-05-26 11:00:00\n\n\n25\n15.0\n2023-05-26 11:00:00\n\n\n\n\n\n\n\n\nWe’ll need to do a groupby operation to summarize the readings for the same county at the same datetime.\nI did the following groupbys on the dates and on the counties:\n\ndate: mean and standard deviation over all counties (and hence all weather stations in each county) at each unique datetime\nlocal: mean and standard deviation over all weather stations in an individual county at each unique datetime, i.e. mean/std of county n at datetime d.\n\nNow that we know how we’ll merge the tables, let’s look at the training set and clean the data."
  },
  {
    "objectID": "posts/enefit-part-1/enefit-1.html#cleaning-the-data",
    "href": "posts/enefit-part-1/enefit-1.html#cleaning-the-data",
    "title": "Enefit - Predict Energy Behavior of Prosumers, part 1",
    "section": "4 Cleaning the data",
    "text": "4 Cleaning the data\n\n\nHide code\nsegment = ['county', 'is_business', 'product_type']\ndata_cols = segment + ['is_consumption', 'datetime', 'row_id', 'target']\nclient_cols = segment + ['eic_count', 'installed_capacity', 'date']\nforecast_cols = ['latitude', 'longitude', 'hours_ahead',\n        'temperature', 'dewpoint', 'cloudcover_high', 'cloudcover_low',\n        'cloudcover_mid', 'cloudcover_total', '10_metre_u_wind_component',\n        '10_metre_v_wind_component', 'forecast_datetime',\n        'direct_solar_radiation', 'surface_solar_radiation_downwards',\n        'snowfall', 'total_precipitation']\nhistorical_cols = ['datetime', 'temperature', 'dewpoint', 'rain', 'snowfall',\n        'surface_pressure', 'cloudcover_total', 'cloudcover_low',\n        'cloudcover_mid', 'cloudcover_high', 'windspeed_10m',\n        'winddirection_10m', 'shortwave_radiation', 'direct_solar_radiation',\n        'diffuse_radiation', 'latitude', 'longitude']\nelectricity_cols = ['forecast_date', 'euros_per_mwh']\ngas_cols = ['forecast_date', 'lowest_price_per_mwh', 'highest_price_per_mwh']\nlocation_cols = ['longitude', 'latitude', 'county']\n\n# prosumer details\ntrain_df = pd.read_csv(path/'train.csv', usecols=data_cols, low_memory=False)\nclient_df = pd.read_csv(path/'client.csv', usecols=client_cols, low_memory=False)\n\n# weather\nforecast_df = pd.read_csv(path/'forecast_weather.csv', usecols=forecast_cols, low_memory=False)\nhistorical_df = pd.read_csv(path/'historical_weather.csv', usecols=historical_cols, low_memory=False)\n\n# energy prices\nelectricity_df = pd.read_csv(path/'electricity_prices.csv', usecols=electricity_cols, low_memory=False)\ngas_df = pd.read_csv(path/'gas_prices.csv', usecols=gas_cols, low_memory=False)\n\n# location\nweather_station_to_county_mapping_df = pd.read_csv(path/'weather_station_to_county_mapping.csv', usecols=location_cols, low_memory=False)\ncounty_id_to_name_map_df = pd.read_json(path/'county_id_to_name_map.json', orient='index')\n\n\n\n\nHide code\ndfs = [train_df, client_df, forecast_df, historical_df, electricity_df, gas_df]\ndf_names = ['train', 'client', 'forecast_weather', 'historical_weather', 'electricity_prices', 'gas_prices']\n\n\nHere are the table shapes:\n\n\nHide code\n{k:v.shape for (k,v) in zip(df_names, dfs)}\n\n\n{'train': (2018352, 7),\n 'client': (41919, 6),\n 'forecast_weather': (3424512, 16),\n 'historical_weather': (1710802, 17),\n 'electricity_prices': (15286, 2),\n 'gas_prices': (637, 3)}\n\n\nWe can expect our merged and cleaned result to have about 2 million rows.\nHere are the columns and their data types:\n\n\nHide code\n{k:dict(v.dtypes) for (k,v) in zip(df_names, dfs)}\n\n\n{'train': {'county': dtype('int64'),\n  'is_business': dtype('int64'),\n  'product_type': dtype('int64'),\n  'target': dtype('float64'),\n  'is_consumption': dtype('int64'),\n  'datetime': dtype('O'),\n  'row_id': dtype('int64')},\n 'client': {'product_type': dtype('int64'),\n  'county': dtype('int64'),\n  'eic_count': dtype('int64'),\n  'installed_capacity': dtype('float64'),\n  'is_business': dtype('int64'),\n  'date': dtype('O')},\n 'forecast_weather': {'latitude': dtype('float64'),\n  'longitude': dtype('float64'),\n  'hours_ahead': dtype('int64'),\n  'temperature': dtype('float64'),\n  'dewpoint': dtype('float64'),\n  'cloudcover_high': dtype('float64'),\n  'cloudcover_low': dtype('float64'),\n  'cloudcover_mid': dtype('float64'),\n  'cloudcover_total': dtype('float64'),\n  '10_metre_u_wind_component': dtype('float64'),\n  '10_metre_v_wind_component': dtype('float64'),\n  'forecast_datetime': dtype('O'),\n  'direct_solar_radiation': dtype('float64'),\n  'surface_solar_radiation_downwards': dtype('float64'),\n  'snowfall': dtype('float64'),\n  'total_precipitation': dtype('float64')},\n 'historical_weather': {'datetime': dtype('O'),\n  'temperature': dtype('float64'),\n  'dewpoint': dtype('float64'),\n  'rain': dtype('float64'),\n  'snowfall': dtype('float64'),\n  'surface_pressure': dtype('float64'),\n  'cloudcover_total': dtype('int64'),\n  'cloudcover_low': dtype('int64'),\n  'cloudcover_mid': dtype('int64'),\n  'cloudcover_high': dtype('int64'),\n  'windspeed_10m': dtype('float64'),\n  'winddirection_10m': dtype('int64'),\n  'shortwave_radiation': dtype('float64'),\n  'direct_solar_radiation': dtype('float64'),\n  'diffuse_radiation': dtype('float64'),\n  'latitude': dtype('float64'),\n  'longitude': dtype('float64')},\n 'electricity_prices': {'forecast_date': dtype('O'),\n  'euros_per_mwh': dtype('float64')},\n 'gas_prices': {'forecast_date': dtype('O'),\n  'lowest_price_per_mwh': dtype('float64'),\n  'highest_price_per_mwh': dtype('float64')}}\n\n\nThe date columns are currently all stored as Python objects:\n\n\nHide code\ndate_cols = ['datetime', 'date', 'forecast_datetime', 'datetime', 'forecast_date', 'forecast_date']\n[(date_cols[i], df[date_cols[i]].dtype) for i,df in enumerate(dfs)]\n\n\n[('datetime', dtype('O')),\n ('date', dtype('O')),\n ('forecast_datetime', dtype('O')),\n ('datetime', dtype('O')),\n ('forecast_date', dtype('O')),\n ('forecast_date', dtype('O'))]\n\n\nWe’ll cast them as a datetime type and check the result:\n\n\nHide code\ndef to_datetime(df:pd.DataFrame, date_col:pd.Series):\n    \"\"\"\n    casts date_col in df as datetime type\n    \"\"\"\n    df[date_col] = pd.to_datetime(df[date_col])\n    return df\n\n# cast\n[to_datetime(df, date_cols[i]) for i,df in enumerate(dfs)]\n# check\n[(date_cols[i], df[date_cols[i]].dtype) for i,df in enumerate(dfs)]\n\n\n[('datetime', dtype('&lt;M8[ns]')),\n ('date', dtype('&lt;M8[ns]')),\n ('forecast_datetime', dtype('&lt;M8[ns]')),\n ('datetime', dtype('&lt;M8[ns]')),\n ('forecast_date', dtype('&lt;M8[ns]')),\n ('forecast_date', dtype('&lt;M8[ns]'))]\n\n\nLet’s check for missing values:\n\n\nHide code\ndef get_columns_with_missing_values(df):\n    # columns with missing values\n    cols = list(df.columns[df.isna().sum() &gt; 0])\n    null_counts = [df[col].isna().sum() for col in cols]\n    return list(zip(cols, null_counts))\n\ndict(zip(df_names, [get_columns_with_missing_values(df) for df in dfs]))\n\n\n{'train': [('target', 528)],\n 'client': [],\n 'forecast_weather': [('surface_solar_radiation_downwards', 2)],\n 'historical_weather': [],\n 'electricity_prices': [],\n 'gas_prices': []}\n\n\nThere are 528 missing targets, which are few enough to drop. We’ll later fill otherwise missing values with the column modes.\n\n\n\n\n\n\nNote\n\n\n\ntrain contains current targets (the ground truth labels) unlike test. At forecast time, we’ll have 2-day old targets available in revealed_targets. We will construct these 48h lagged targets in later iterations when we do more extensive feature engineering.\n\n\nNext we’ll look for highly skewed columns, particularly those distributed with a long tail to the right. While decision trees can handle these just fine, they can pose problems for models that multiply the input data by a coefficient, like a neural network. This is because the infrequently occurring large numbers can dominate the result.\n\n\nHide code\ndef get_skewed_cols(df:pd.DataFrame):\n    df_skew = pd.DataFrame(df.skew(), columns=['skew'])\n    skewed_cols = list(df_skew[np.abs(df_skew['skew']) &gt;= 0.75].index)\n    return skewed_cols\n\n# look at the skewed columns first b/c could include columns you don't want to log-scale, e.g. segment cols, target.\ndict(zip(df_names, [get_skewed_cols(df) for df in dfs]))\n\n\n{'train': ['target'],\n 'client': ['eic_count', 'installed_capacity'],\n 'forecast_weather': ['direct_solar_radiation',\n  'surface_solar_radiation_downwards',\n  'snowfall',\n  'total_precipitation'],\n 'historical_weather': ['rain',\n  'snowfall',\n  'windspeed_10m',\n  'shortwave_radiation',\n  'direct_solar_radiation',\n  'diffuse_radiation'],\n 'electricity_prices': ['euros_per_mwh'],\n 'gas_prices': ['lowest_price_per_mwh', 'highest_price_per_mwh']}\n\n\nWe can visualize some with histograms. Here’s the target:\n\n\nHide code\ntrain_df['target'].hist()\n\n\n\n\n\n\n\n\n\n\n\nHide code\nclient_df['installed_capacity'].hist()\n\n\n\n\n\n\n\n\n\nAmong the columns we found, we’ll log scale all but the target to squish the large numbers and reduce the skew:\n\n\nHide code\nskewed_cols = [\n    ['eic_count', 'installed_capacity'], \n    ['direct_solar_radiation', 'surface_solar_radiation_downwards', 'snowfall', 'total_precipitation'], \n    ['rain', 'snowfall', 'windspeed_10m', 'shortwave_radiation', 'direct_solar_radiation', 'diffuse_radiation'], \n    ['euros_per_mwh'], \n    ['lowest_price_per_mwh', 'highest_price_per_mwh']\n]\n\n\n\n\nHide code\ndef log_scale_df(df:pd.DataFrame, skewed_cols:list):\n    # add log scaled columns\n    for col in skewed_cols:\n        df[f\"log_{col}\"] = np.log(df[col]+1)\n        \n    # drop the original skewed columns\n    df.drop(columns=skewed_cols, inplace=True)\n    return df\n\n# not logging target\nfor i, (df, cols) in enumerate(zip(dfs[1:], skewed_cols)):\n    log_scale_df(df, cols)\n\n\nHere’s the result for installed_capacity:\n\n\nHide code\nclient_df['log_installed_capacity'].hist()\n\n\n\n\n\n\n\n\n\nNow we’ll merge the tables:\n\n\nShow code\ndef add_client_features(features_df, client_df): \n    \n    # to align w/ forecast horizon, shift dates 2 days forward (equivalent to 48h lag)\n    client_df['date'] = client_df['date'] + pd.Timedelta(days=2)\n    \n    # create date column in train_df to join on (will drop after)\n    features_df['date'] = pd.to_datetime(features_df['datetime'].dt.date)\n    \n    # join\n    features_df = features_df.merge(client_df, how='left', on=['county', 'is_business', 'product_type', 'date'])\n    features_df.drop(columns=['date'], axis=1, inplace=True)\n    \n    return features_df\n\ndef _reduce_float64(df):\n    float64_cols = df.select_dtypes(include=['float64']).columns.tolist()\n    for col in float64_cols:\n        df[col] = df[col].astype('float32')\n    return df\n\ndef add_forecast_weather_features(features_df, forecast_df, weather_station_to_county_mapping_df):\n    # setup\n    # rename forecast_datetime left-join w/ features_df on datetime\n    forecast_df.rename(columns={'forecast_datetime': 'datetime'}, inplace=True)\n\n    # to align w/ forecast horizon, filter by 22-45 hours ahead \n    forecast_df = forecast_df.loc[(forecast_df['hours_ahead'] &gt;= 22) & (forecast_df['hours_ahead'] &lt;= 45)].copy()\n    forecast_df.drop(columns=['hours_ahead'], axis=1, inplace=True)\n\n    # add county ID by merging w/ weather_station_to_county_mapping_df\n    forecast_df = forecast_df.merge(\n        weather_station_to_county_mapping_df, how='left', on=['longitude', 'latitude'])\n\n    # fill NaN counties w/ 12: code for 'Unknown' county according to county_id_to_name_map.json\n    forecast_df['county'] = forecast_df['county'].fillna(12).astype('int64') \n\n    # drop longitude, latitude\n    forecast_df.drop(columns=['longitude', 'latitude'], axis=1, inplace=True)\n\n    ####### date stats: stats over all counties (and all weather stations in each county) on each unique datetime #######\n    # date - mean over all counties on each unique date. drop county b/c left-joining back on 'datetime' only\n    date_mean = forecast_df.groupby(['datetime'], as_index=False).mean()\n    date_mean.drop(columns=['county'], axis=1, inplace=True) \n    # std\n    date_std = forecast_df.groupby(['datetime'], as_index=False).std()\n    date_std.drop(columns=['county'], axis=1, inplace=True)\n\n    ####### local stats: stats over a county on each unique datetime (mean of county N on date YY-MM-DD) #######\n    # filter where counties are known each time first and cast county as int to enable left-join\n    # mean\n    local_mean = forecast_df[~forecast_df['county'].isna()].groupby(['county', 'datetime'], as_index=False).mean()\n    local_mean['county'] = local_mean['county'].fillna(12).astype('int64')\n    # std\n    local_std = forecast_df[~forecast_df['county'].isna()].groupby(['county', 'datetime'], as_index=False).std()\n    local_std['county'] = local_std['county'].fillna(12).astype('int64')\n    ##\n    \n    # reduce memory usage of intermediate dfs by casting float64 as float32\n    [_reduce_float64(df) for df in [date_mean, date_std, local_mean, local_std]]\n    \n    # join date stats\n    features_df = features_df.merge(date_mean, how='left', on=['datetime'])\n    features_df = features_df.merge(date_std, how='left', on=['datetime'], suffixes=(\"_mean_fd\", \"_std_fd\"))\n    #print(\"debug - merged all date forecast stats\")\n\n    # join local stats\n    features_df = features_df.merge(local_mean, how='left', on=['county', 'datetime'])\n    features_df = features_df.merge(local_std, how='left', on=['county', 'datetime'], suffixes=(\"_mean_fl\", \"_std_fl\"))\n    #print(\"debug - merged all local forecast stats\")\n\n    del date_mean, date_std, local_mean, local_std\n    gc.collect()\n    #print(\"debug - finished adding forecast weather features\")\n    return features_df\n\ndef add_historical_weather_features(features_df, historical_df, weather_station_to_county_mapping_df):\n\n    ## setup\n    # add county ID by merging w/ weather_station_to_county_mapping_df\n    historical_df = historical_df.merge(\n        weather_station_to_county_mapping_df, how='left', on=['longitude', 'latitude'])\n\n    # fill NaN counties w/ 12: code for Unknown county according to county_id_to_name_map.json\n    historical_df['county'] = historical_df['county'].fillna(12).astype('int64')\n\n    # drop longitude, latitude \n    historical_df.drop(columns=['longitude', 'latitude'], axis=1, inplace=True)\n\n    ####### date stats: stats over all counties on each unique date #######\n    ## base date stats. drop county b/c joining on 'datetime' only ##\n    # mean\n    date_mean = historical_df.groupby(['datetime'], as_index=False).mean()\n    date_mean.drop(columns=['county'], axis=1, inplace=True) \n    # std\n    date_std = historical_df.groupby(['datetime'], as_index=False).std()\n    date_std.drop(columns=['county'], axis=1, inplace=True)\n\n    ####### local stats: stats over a county on each unique date (mean of county N on date YY-MM-DD) #######\n    ## base local stats. to join on county, datetime: filter where counties are known each time first and cast county as int ## \n    # mean\n    local_mean = historical_df[~historical_df['county'].isna()].groupby(['county', 'datetime'], as_index=False).mean()\n    local_mean['county'] = local_mean['county'].fillna(12).astype('int64')\n    # std\n    local_std = historical_df[~historical_df['county'].isna()].groupby(['county', 'datetime'], as_index=False).std()\n    local_std['county'] = local_std['county'].fillna(12).astype('int64')\n    ##\n    \n    # cast float64 as float32 to reduce memory usage\n    [_reduce_float64(df) for df in [date_mean, date_std, local_mean, local_std]]\n\n    # join all date stats\n    features_df = features_df.merge(date_mean, how='left', on=['datetime'])\n    features_df = features_df.merge(date_std, how='left', on=['datetime'], suffixes=(\"_mean_hd\", \"_std_hd\"))\n\n    # join all local stats\n    features_df = features_df.merge(local_mean, how='left', on=['county', 'datetime'])\n    features_df = features_df.merge(local_std, how='left', on=['county', 'datetime'], suffixes=('_mean_hl', '_std_hl'))\n\n    del date_mean, date_std, local_mean, local_std\n    gc.collect()\n    return features_df\n\ndef add_electricity_prices_features(features_df, electricity_df):\n    ## setup\n    # rename forecast_date left-join w/ features_df on 'datetime'\n    electricity_df.rename(columns={'forecast_date': 'datetime'}, inplace=True)\n    \n    # to align w/ forecast horizon, shift dates forward 1 day (equivalent to 24h lag)\n    electricity_df['datetime'] = electricity_df['datetime'] + pd.Timedelta(days=1)\n    ##\n    \n    # join\n    features_df = features_df.merge(electricity_df, how='left', on=['datetime'])\n    \n    return features_df\n\ndef add_gas_prices_features(features_df, gas_df):\n    ## setup:\n    # create date column in features_df to join on (will drop after) \n    features_df['date'] = pd.to_datetime(features_df['datetime'].dt.date)\n    \n    # rename forecast_date in gas_df to join on\n    gas_df.rename(columns={'forecast_date': 'date'}, inplace=True)\n    \n    # to align w/ forecast horizon, shift dates forward 1 day (equivalent to 24h lag)\n    gas_df['date'] = gas_df['date'] + pd.Timedelta(days=1)\n    ##\n    \n    # join\n    features_df = features_df.merge(gas_df, how='left', on=['date'])\n    \n    # drop date\n    features_df.drop(columns=['date'], axis=1, inplace=True)\n    \n    return features_df\n\n\n\n\nHide code\nfeatures_df = add_client_features(train_df, client_df)\nfeatures_df = add_forecast_weather_features(features_df, forecast_df, weather_station_to_county_mapping_df)\nfeatures_df = add_historical_weather_features(features_df, historical_df, weather_station_to_county_mapping_df)\nfeatures_df = add_electricity_prices_features(features_df, electricity_df)\nfeatures_df = add_gas_prices_features(features_df, gas_df)\n\n\nAt this stage, the only feature engineering we’ll do is enriching the representation of dates with metadata like DayofWeek/DayofMonth, is_month_start/end, is_weekend, is_holiday, etc; and capturing periodicity to distinguish hours of the same day or months of the same year.\n\n\nShow code\ndef add_is_holiday_flag(features_df):\n    # supports years 2021-2026, up to 2 years past competition's approx forecast horizon--approx May 2024\n    estonian_holidays = holidays.country_holidays('EE', years=[2021, 2022, 2023, 2024, 2025, 2026])\n    features_df['is_holiday'] = [date in estonian_holidays for date in features_df['datetime'].dt.date]\n\n    return features_df\n\ndef add_is_weekend_flag(features_df):\n    \"\"\"\n    must call after calling add_datepart\n    add_datepart encodes Dayofweek as #'s b/w 0-6: \n    {0: monday, 1: tuesday ... 5: saturday, 6: sunday}\n    \"\"\"\n    weekends = [5, 6]\n    features_df['is_weekend'] = [day in weekends for day in features_df['datetimeDayofweek']]\n    \n    return features_df\n\ndef add_date_features(features_df):\n    # add Year, Month, Week (of year), Day (of month), Dayof{week, year}, is_{start, end} of {month, quarter, year}\n    features_df = add_datepart(features_df, 'datetime', drop=False)\n    \n    # add is_weekend flag\n    features_df = add_is_weekend_flag(features_df)\n    \n    # add is_holiday flag\n    features_df = add_is_holiday_flag(features_df)\n    \n    # capture orthogonal components (peaks/troughs vs phase shift) of Dayofyear, Hour\n    # helps distinguish different months in the same year, hours in the same day\n    # sin emphasizes vertical diplacement (peaks and troughs)\n    # cos emphasizes horizontal displacement (phase shift)\n    features_df['datetimeHour'] = features_df['datetime'].dt.hour\n    features_df['sin(datetimeDayofyear)'] = np.sin(np.pi * features_df['datetimeDayofyear'] / 183)\n    features_df['sin(datetimeHour)'] = np.sin(np.pi * features_df['datetimeHour'] / 12) \n    features_df['cos(datetimeDayofyear)'] = np.sin(np.pi * features_df['datetimeDayofyear'] / 183)\n    features_df['cos(datetimeHour)'] = np.sin(np.pi * features_df['datetimeHour'] / 12) \n\n    return features_df\n\n\n\n\nHide code\nfeatures_df = add_date_features(features_df)\nfeatures_df.shape, features_df.columns\n\n\n((2018352, 136),\n Index(['county', 'is_business', 'product_type', 'target', 'is_consumption',\n        'datetime', 'row_id', 'log_eic_count', 'log_installed_capacity',\n        'temperature_mean_fd',\n        ...\n        'datetimeIs_year_end', 'datetimeIs_year_start', 'datetimeElapsed',\n        'is_weekend', 'is_holiday', 'datetimeHour', 'sin(datetimeDayofyear)',\n        'sin(datetimeHour)', 'cos(datetimeDayofyear)', 'cos(datetimeHour)'],\n       dtype='object', length=136))\n\n\nThe dependent variable is target.\n\n\nHide code\ndep_var = 'target'\n\n\nAs mentioned earlier, we’ll filter out rows with missing targets and fill otherwise missing values with the column modes.\n\n\nHide code\nfeatures_df = features_df[features_df[dep_var].notnull()]\nmodes = features_df.mode().iloc[0].copy()\nfeatures_df.fillna(modes, inplace=True)\n\n\nHere’s the result:\n\n\nHide code\nget_columns_with_missing_values(df)\n\n\n[]\n\n\n\n\nHide code\nfeatures_df.head()\n\n\n\n\n\n\n\n\n\n\ncounty\nis_business\nproduct_type\ntarget\nis_consumption\n...\ndatetimeHour\nsin(datetimeDayofyear)\nsin(datetimeHour)\ncos(datetimeDayofyear)\ncos(datetimeHour)\n\n\n\n\n0\n0\n0\n1\n0.713\n0\n...\n0\n-0.866025\n0.0\n-0.866025\n0.0\n\n\n1\n0\n0\n1\n96.590\n1\n...\n0\n-0.866025\n0.0\n-0.866025\n0.0\n\n\n2\n0\n0\n2\n0.000\n0\n...\n0\n-0.866025\n0.0\n-0.866025\n0.0\n\n\n3\n0\n0\n2\n17.314\n1\n...\n0\n-0.866025\n0.0\n-0.866025\n0.0\n\n\n4\n0\n0\n3\n2.904\n0\n...\n0\n-0.866025\n0.0\n-0.866025\n0.0\n\n\n\n\n5 rows × 136 columns\n\n\n\n\nExcluding the target, 135 columns is a lot to study at once. As we go on, we’ll narrow our focus to the most influential ones.\nNext we’ll split the data to use Sept 2021 - Jan 2023 for training and Feb 23 for validation. From the training set, we’ll drop a random set–and a random amount–of prosumers among those that are common to both training and validation just after splitting, making the validation ones new from the model’s point of view. We’ll indicate how many and which ones we drop with their prediction_unit_id’s (PIDs).\nfastai’s TabularPandas class will handle splitting the data as well as encoding continuous and categorical variables.\n\n\nShow code\nfrom numpy import random\nrandom.seed(42)\n\nURLs.LOCAL_PATH = Path('/notebooks/kaggle/enefit')\nsave_path = URLs.LOCAL_PATH/'predict-energy-behavior-of-prosumers/pkl_files'\npid_lookup_df = load_pickle(save_path/'pid_lookup.pkl')\n\n# includes dates of revealed_targets (2 days behind first eval day feb 1, 2023)\ntrain_dates = {\n    'part': [pd.to_datetime('2022-11-01 00:00:00'), pd.to_datetime('2023-01-29 23:00:00')],\n    'full': [pd.to_datetime('2021-09-01 00:00:00'), pd.to_datetime('2023-01-29 23:00:00')]\n}\n\neval_dates = {'feb23': [pd.to_datetime('2023-02-01 00:00:00'), pd.to_datetime('2023-02-28 23:00:00')],\n              'mar23': [pd.to_datetime('2023-03-01 00:00:00'), pd.to_datetime('2023-03-31 23:00:00')],\n              'apr23': [pd.to_datetime('2023-04-01 00:00:00'), pd.to_datetime('2023-04-30 23:00:00')],\n              'may23': [pd.to_datetime('2023-05-01 00:00:00'), pd.to_datetime('2023-05-31 23:00:00')]\n             }\n\ndef get_endpoints(is_partial, features_df, eval_month:str):\n\n    if is_partial:\n        train_dates_conds = [features_df['datetime'] == train_dates['part'][0], \n                             features_df['datetime'] == train_dates['part'][1]\n                            ]\n    else:\n        train_dates_conds = [features_df['datetime'] == train_dates['full'][0], \n                             features_df['datetime'] == train_dates['full'][1]\n                            ]\n    \n    train_endpoints = [features_df['datetime'][train_dates_conds[0]].index[0], \n                       features_df['datetime'][train_dates_conds[1]].index[-1]\n                      ]\n\n    valid_dates_cond = [features_df['datetime'] == eval_dates[eval_month][0], \n                        features_df['datetime'] == eval_dates[eval_month][1]\n                       ]\n\n    valid_endpoints = [features_df['datetime'][valid_dates_cond[0]].index[0], \n                       features_df['datetime'][valid_dates_cond[1]].index[-1]\n                      ]\n\n    return train_endpoints, valid_endpoints\n\ndef get_splits(is_partial, features_df, eval_month:str):\n    train_endpoints, valid_endpoints = get_endpoints(is_partial, features_df, eval_month)\n    train = features_df.iloc[train_endpoints[0]:train_endpoints[1]+1].index.tolist()\n    valid = features_df.iloc[valid_endpoints[0]:valid_endpoints[1]+1].index.tolist()\n    splits = (train, valid)\n    return splits\n\ndef drop_pids_from_train(features_df, splits, pid_lookup_df):\n    \n    max_n_pids_to_drop = 7\n    training_pids = set(features_df.iloc[splits[0]].merge(pid_lookup_df, how='left', on=['row_id'])['prediction_unit_id'].unique().tolist())\n    valid_pids = set(features_df.iloc[splits[1]].merge(pid_lookup_df, how='left', on=['row_id'])['prediction_unit_id'].unique().tolist())\n    intersection_pids = list(training_pids.intersection(valid_pids))\n    \n    # pids drop list may not be continuous or sorted.\n    intersection_pids.sort() \n\n    # drop from those not already missing in the valid fold, THIS ROUND.\n    # to get the number of pids to drop THIS ROUND, pick a random int b/w 1 and the max allowed to drop.\n    n_pids_drop = random.randint(1, max_n_pids_to_drop) \n\n    # to get the pids to drop, index intersection_pids w/ a random int b/w 0 and its last index. repeat for the n_pids_drop chosen for THIS ROUND. \n    pids_to_drop = [random.randint(0, len(intersection_pids)-1) for i in range(n_pids_drop)]\n\n    # drop only from train (slice features_df by training dates from the endpoints, join to lookup pids, mask, apply the mask to the slice)\n    # get its indices\n    mask = features_df.iloc[splits[0]].merge(pid_lookup_df, how='left', on=['row_id'])['prediction_unit_id'].isin(pids_to_drop)\n\n    return mask, n_pids_drop, pids_to_drop\n\ndef setup_tabular_pandas(features_df, procs, max_card, splits, dep_var='target'):  \n    # variables\n    cont, cat = cont_cat_split(features_df, max_card, dep_var=dep_var)\n\n    # setup object\n    return TabularPandas(features_df, procs, cat, cont, y_names=dep_var, splits=splits)\n\ndef setup_training(is_partial, features_df, pid_lookup_df, eval_month, procs, max_card):\n\n    features_df = features_df.reset_index(drop=True)      \n    tmp_splits = get_splits(is_partial, features_df, eval_month)\n\n    # _get_splits -&gt; _get_pids_drop_mask needs row_id to join pid_lookup_df\n    pid_mask, n_pids_dropped, pids_dropped = drop_pids_from_train(features_df, tmp_splits, pid_lookup_df)\n    valid_buffer = pd.Series(np.zeros(len(tmp_splits[1]), dtype='bool'))\n    pid_mask = pd.concat([pid_mask, valid_buffer])\n    pid_mask.index = tmp_splits[0] + tmp_splits[1]\n    tmp_df = features_df.iloc[tmp_splits[0]+tmp_splits[1]][~pid_mask]\n    features_df_final = tmp_df.reset_index(drop=True)\n\n    # re-compute the splits after applying pid_mask.\n    final_splits = get_splits(is_partial, features_df_final, eval_month)\n    del tmp_splits, tmp_df\n    gc.collect()\n\n    # setup training\n    procs = procs\n    max_card = max_card\n    to = setup_tabular_pandas(features_df_final, procs, max_card, final_splits)\n\n    print(f\"n_pids_dropped from train: {n_pids_dropped}\")\n    print(f\"pids_dropped from train: {pids_dropped}\")\n    \n    return to, final_splits\n\n\n\n\nHide code\nis_partial = False\neval_month = 'feb23'\nprocs = [Categorify]\nmax_card = 20\n\nto, final_splits = setup_training(is_partial, features_df, pid_lookup_df, eval_month, procs, max_card)\n\n\nn_pids_dropped from train: 4\npids_dropped from train: [14, 60, 20, 23]"
  },
  {
    "objectID": "posts/enefit-part-1/enefit-1.html#modeling-with-trees",
    "href": "posts/enefit-part-1/enefit-1.html#modeling-with-trees",
    "title": "Enefit - Predict Energy Behavior of Prosumers, part 1",
    "section": "5 Modeling with trees",
    "text": "5 Modeling with trees\nWith our pre-processing done, there are no missing values and the data is all numeric. We’re ready to train a model! Since decision trees are easy to train and interpret, we’ll start there.\nLet’s define our independent and dependent variables:\n\n\nHide code\ndef get_train_and_valid(to):\n    xs, y = to.train.xs, to.train.y\n    valid_xs, valid_y = to.valid.xs, to.valid.y\n    \n    return xs, y, valid_xs, valid_y\n\nxs, y, valid_xs, valid_y = get_train_and_valid(to)\nprint(len(xs), len(y), len(valid_xs), len(valid_y))\n\n\n1531020 1531020 89808 89808\n\n\nAnd train a small tree with sklearn, limiting to 4 leaf nodes for simplicity:\n\n\nHide code\n%%time \nm = DecisionTreeRegressor(max_leaf_nodes=4)\nm.fit(xs, y);\n\n\nCPU times: user 28.1 s, sys: 937 ms, total: 29 s\nWall time: 29 s\n\n\nDecisionTreeRegressor(max_leaf_nodes=4)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_leaf_nodes=4)\n\n\nfastai’s draw_tree function displays what the tree learned:\n\n\nHide code\ndraw_tree(m, xs, size=15, leaves_parallel=True, precision=2)\n\n\n\n\n\n\n\n\nFigure 1: Visualizing the decision criteria\n\n\n\n\n\nThe top node is the initial model before any splits have been made. The model has the data all in one group (samples = 1531020—the total size of the training set), and always predicts the target to be its average value over the whole dataset (254.05). The mean squared error between this value and the actual targets is 805354.01. The tree-building algorithm found the best split to be on log_installed_capacity, with the decision criteria (a Yes or No question) being whether log_installed_capacity is less than or equal to 9.41.\nThe node moving down to the left shows that there are 1503570 rows where log_installed_capacity is less than or equal to 9.41. In this group, the average value of the target is 186.89. The best next split was found to be whether log_installed_capacity is less than or equal to 8.59.\nIf we instead moved down to the right, we have rows where log_installed_capacity is greater than 9.41. The best next split from here is on is_consumption.\nThe nodes on the bottom row are the leaf nodes; no further questions are asked. We can see that with each split, the model successfully separates larger targets from smaller targets, and that the resulting groups’ average values differ significantly.\nWith Terrence Parr’s dtreeviz library, we can gain further insight from the same model:\n\n\nHide code\n# sample 1% points\nimport dtreeviz\nsamp_idx = np.random.permutation(len(y))\nsamp_idx.sort()\nsamp_idx = samp_idx[:20000]\n\ntreeviz = dtreeviz.model(m, xs.iloc[samp_idx], y.iloc[samp_idx], feature_names=xs.columns, target_name=dep_var)\ntreeviz.view(fontname='DejaVu Sans', scale=1.6, label_fontsize=10, orientation='LR')\n\n\n\n\n\n\n\n\nFigure 2: Visualizing the target distribution after each split\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTabularPandas encodes categorical variables by replacing each unique level with a number. The numbers for each level are chosen consecutively based on the level’s order of appearance, but they have no particular meaning.\n\n\nFigure 2 shows the target distribution at each split point. The targets splitting on is_consumption have quite different distributions, which is in turn reflected in the leaf nodes: the average production (1) is 492.46, and the average consumption (2) is 7373.03. Perhaps it’s worth investigating the groups more closely in a later iteration, even modeling them separately.\nIn is_consumption, 0 (production) appears first and 1 (consumption) appears second, as seen before the TabularPandas was created:\n\n\nHide code\nfeatures_df[segment + ['is_consumption', 'datetimeYear', 'datetimeMonth', 'datetimeDay', 'datetimeHour']].head(3)\n\n\n\n\n\n\n\n\n\n\ncounty\nis_business\nproduct_type\nis_consumption\ndatetimeYear\ndatetimeMonth\ndatetimeDay\ndatetimeHour\n\n\n\n\n0\n0\n0\n1\n0\n2021\n9\n1\n0\n\n\n1\n0\n0\n1\n1\n2021\n9\n1\n0\n\n\n2\n0\n0\n2\n0\n2021\n9\n1\n0\n\n\n\n\n\n\n\n\nWe can see TabularPandas encoded production and consumption as 1 and 2 respectively:\n\n\nHide code\nxs[:3][segment + ['is_consumption', 'datetimeYear', 'datetimeMonth', 'datetimeDay', 'datetimeHour']]\n\n\n\n\n\n\n\n\n\n\ncounty\nis_business\nproduct_type\nis_consumption\ndatetimeYear\ndatetimeMonth\ndatetimeDay\ndatetimeHour\n\n\n\n\n0\n1\n1\n2\n1\n1\n9\n1\n0\n\n\n1\n1\n1\n2\n2\n1\n9\n1\n0\n\n\n2\n1\n1\n3\n1\n1\n9\n1\n0\n\n\n\n\n\n\n\n\nWe need to evaluate the models as we go on. The competition is scored on mean absolute error (MAE), so we’ll create functions to calculate it:\n\n\nHide code\ndef mae(preds, y): return round((preds-y).abs().mean(), 6)\ndef m_mae(m, xs, y): return mae(m.predict(xs), y)\n\n\nLet’s train a larger tree:\n\n\nHide code\n%%time\nm = DecisionTreeRegressor(min_samples_leaf=25)\nm.fit(to.train.xs, to.train.y)\n\n\nCPU times: user 4min 16s, sys: 1.41 s, total: 4min 17s\nWall time: 4min 18s\n\n\nDecisionTreeRegressor(min_samples_leaf=25)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(min_samples_leaf=25)\n\n\n\n\nHide code\nprint(\"train mae: \", m_mae(m, xs, y))\nprint(\"valid mae: \", m_mae(m, valid_xs, valid_y))\n\n\ntrain mae:  22.86435\nvalid mae:  82.516897\n\n\n\n\nHide code\nm.get_n_leaves(), len(xs)\n\n\n(41727, 1531020)\n\n\nAs shown, the training error is quite a bit less than the validation error, so this model is overfitting. However, with ~42000 leaves vs ~1.5 million rows, it’s apparently not because the leaf nodes are near as many as the number of rows. If that was the case, the tree has essentially become a perfect charades guesser: it simply describes the rows rather than having learned underlying patterns about them.\nThere’s a tradeoff between how accurate the tree is on the training set and how well it generalizes. Smaller trees generalize better, but larger trees are more accurate. A random forest brings the best of both worlds while also being easy to train and interpret. Let’s train one next.\n\n\nHide code\ndef rf(xs, y, n_estimators=40, max_features=0.5, min_samples_leaf=5, **kwargs):\n    return RandomForestRegressor(n_jobs=-1, \n                                 n_estimators=n_estimators,\n                                 max_samples=int(0.5*len(xs)), \n                                 max_features=max_features,\n                                 min_samples_leaf=min_samples_leaf, \n                                 oob_score=True).fit(xs, y)\n\n\n\n\nHide code\n%%time\nm = rf(xs, y)\n\n\nCPU times: user 53min 22s, sys: 4.25 s, total: 53min 27s\nWall time: 9min 24s\n\n\n\n\nHide code\nprint(\"train mae: \", m_mae(m, xs, y))\nprint(\"valid mae: \", m_mae(m, valid_xs, valid_y))\nprint(\"OOB error: \", mae(m.oob_prediction_, y))\n\n\ntrain mae:  16.108207\nvalid mae:  57.444592\nOOB error:  21.931264\n\n\nOur random forest is also overfitting, but the validation error improved a fair bit compared to the decision tree’s 82.5.\nA random forest is a ensemble of decision trees constructed by bagging: several decision trees are trained on a random subset of rows and columns. We trained 40 trees each on random choices of half the rows and half the columns. The randomness is meant to make the trees independent from one another so that their errors are also independent. Some trees overstimate the target while others underestimate it. Berkeley professor Leo Breiman, the inventor of bagging, had the insight that such independent predictors’ average error will tend to 0.\n\n\n\n\n\n\nNote\n\n\n\nBagging reflects the “wisdom of crowds”, the idea that large groups of people are collectively smarter than individual experts. But it assumes that the individuals think for themselves and aren’t easily influenced by others. If the trees in the random forest were all built on the same subset of rows and columns, they’d all make the same decisions (as well as mistakes) in the same ways, reflecting instead the “madness of crowds”. Bagging helps ensure the crowd is wise rather than just insane.\n\n\nThe out-of-bag (OOB) error is the average error across all trees on rows that each individual tree wasn’t trained on, as if these rows were the tree’s own validation set. Since the OOB error is significantly lower than the ensemble’s validation error, the individual trees are not overfitting their training subsets badly enough to drive the ensemble’s error. Something is affecting the ensemble’s performance, in addition to normal overfitting. One factor is likely the domain shift inherent in time series forecasting: training data is older than validation data and may exhibit patterns that aren’t valid anymore.\nSome prosumers may be especially tricky to model. To discover hard examples, we can assess a prediction’s relative confidence: its standard deviation across all trees. Let’s look at relative confidence for the first 5 validation examples:\n\n\nHide code\n# get the predictions of each tree (40 trees, 89808 preds per tree)\n# 40 tensors, each a rank 1 tensor (vector) of 7988 preds (7988 rows)\npreds = np.stack([t.predict(valid_xs) for t in m.estimators_])\n\n# get std of preds over all trees\npreds_std = preds.std(0)\n\n# std for first 5 targets\npreds_std[:5]\n\n\narray([ 56.83899946, 189.77693871,   1.63845795,   6.62099179,   4.81058034])\n\n\nThe confidence varies widely. Let’s take a closer look:\n\n\nHide code\nvalid_xs.merge(pid_lookup_df, how='left', on=['row_id'])[segment + ['is_consumption', 'prediction_unit_id']][:5]\n\n\n\n\n\n\n\n\n\n\ncounty\nis_business\nproduct_type\nis_consumption\nprediction_unit_id\n\n\n\n\n0\n1\n1\n2\n1\n0\n\n\n1\n1\n1\n2\n2\n0\n\n\n2\n1\n1\n3\n1\n1\n\n\n3\n1\n1\n3\n2\n1\n\n\n4\n1\n1\n4\n1\n2\n\n\n\n\n\n\n\n\nPID 0 seems to have much higher standard deviations (lower relative confidence) than PIDs 1 and 2. It’s predictions for consumption (2) are much less confident than for production (1) as well. We can get a broader sense of relative confidence from its quartiles:\n\n\nHide code\npd.DataFrame(preds_std).describe()\n\n\n\n\n\n\n\n\n\n\n0\n\n\n\n\ncount\n89808.000000\n\n\nmean\n102.017488\n\n\nstd\n224.477415\n\n\nmin\n0.001592\n\n\n25%\n4.270308\n\n\n50%\n16.668092\n\n\n75%\n97.181082\n\n\nmax\n2899.394995\n\n\n\n\n\n\n\n\nWe can see that the predictions’ standard deviations range from 0 (most confident) to ~2900 (least confident), with the middle 50% between ~4 and ~97.\nTo zero in on the hardest examples, we’ll break down the sample distribution in the top quartile:\n\n\nHide code\n[len(np.where(preds_std &gt;= threshold)[0]) for threshold in [97, 1000, 1500, 2000, 2500, 2600, 2700, 2800]]\n\n\n[22475, 1479, 416, 75, 33, 19, 7, 1]\n\n\nThe hardest 7 examples have a standard deviation of at least 2700. With the provided county_id_to_name_map, we can view them alongside the county_names.\n\n\nHide code\ncounty_id_to_name_map_df.insert(loc=0, column='county', value=county_id_to_name_map_df.index)\ncounty_id_to_name_map_df['county'] = county_id_to_name_map_df['county'] + 1\ncounty_id_to_name_map_df.columns = ['county', 'county_name']\ncounty_id_to_name_map_df.head()\n\n\n\n\n\n\n\n\n\n\ncounty\ncounty_name\n\n\n\n\n0\n1\nHARJUMAA\n\n\n1\n2\nHIIUMAA\n\n\n2\n3\nIDA-VIRUMAA\n\n\n3\n4\nJÄRVAMAA\n\n\n4\n5\nJÕGEVAMAA\n\n\n\n\n\n\n\n\n\n\nHide code\ndate_meta_cols = ['datetimeYear', 'datetimeMonth', 'datetimeHour', 'is_weekend']\n\n\nHere’s the hardest example:\n\n\nHide code\ncond = np.where(preds_std &gt;= 2800)\n\nvalid_xs.iloc[cond\n].merge(\n    pid_lookup_df, how='left', on=['row_id']\n).merge(\n    county_id_to_name_map_df, how='left', on=['county']\n)[\n    segment + ['is_consumption', 'prediction_unit_id', 'county_name'] + date_meta_cols\n]\n\n\n\n\n\n\n\n\n\n\ncounty\nis_business\nproduct_type\nis_consumption\nprediction_unit_id\ncounty_name\ndatetimeYear\ndatetimeMonth\ndatetimeHour\nis_weekend\n\n\n\n\n0\n1\n2\n4\n2\n5\nHARJUMAA\n3\n2\n14\n1\n\n\n\n\n\n\n\n\nTabularPandas encoded the date columns above as follows:\n\ndatetimeYear: years 2021, 2022 and 2023 as 1, 2, and 3, respectively\ndatetime_Month: months Jan-Dec as 1-12, respectively\ndatetime_hour: the start of each hour in a day (12am-11pm) as 0-23, respectively\nis_weekend: False as 1 and True as 2\n\nSo it’s a consumption target in Harjumaa, at 2pm on a weekday in Feb 2023.\nHere are the 7 hardest examples:\n\n\nHide code\ncond = (preds_std &gt;= 2700)\n\nvalid_xs.iloc[cond\n].merge(\n    pid_lookup_df, how='left', on=['row_id']\n).merge(\n    county_id_to_name_map_df, how='left', on=['county']\n)[\n    segment + ['is_consumption', 'prediction_unit_id', 'county_name'] + date_meta_cols\n]\n\n\n\n\n\n\n\n\n\n\ncounty\nis_business\nproduct_type\nis_consumption\nprediction_unit_id\ncounty_name\ndatetimeYear\ndatetimeMonth\ndatetimeHour\nis_weekend\n\n\n\n\n0\n1\n2\n4\n2\n5\nHARJUMAA\n3\n2\n7\n1\n\n\n1\n1\n2\n4\n2\n5\nHARJUMAA\n3\n2\n13\n1\n\n\n2\n1\n2\n4\n2\n5\nHARJUMAA\n3\n2\n15\n1\n\n\n3\n1\n2\n4\n2\n5\nHARJUMAA\n3\n2\n12\n1\n\n\n4\n1\n2\n4\n2\n5\nHARJUMAA\n3\n2\n13\n1\n\n\n5\n1\n2\n4\n2\n5\nHARJUMAA\n3\n2\n14\n1\n\n\n6\n1\n2\n4\n2\n5\nHARJUMAA\n3\n2\n17\n1\n\n\n\n\n\n\n\n\nInteresting. They’re all consumption targets in Harjumaa, mostly between 1-5pm on weekdays in Feb 2023.\n\n\nHide code\ncond = (preds_std &gt;= 1000)\n\nvalid_xs.iloc[cond\n].merge(\n    pid_lookup_df, how='left', on=['row_id']\n).merge(\n    county_id_to_name_map_df, how='left', on=['county']\n)[\n    segment + ['is_consumption', 'prediction_unit_id', 'county_name']\n]['county_name'].unique()\n\n\narray(['HARJUMAA', 'VÕRUMAA', 'TARTUMAA'], dtype=object)\n\n\nThe hardest examples (top ~1.7%) are in Harjumaa, Võrumaa, and Tartumaa.\nNext, let’s look at what features were found to be the strongest predictors with feature importances:\n\n\nHide code\ndef rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)\n\ndef plot_fi(fi):\n    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\n\n\n\nHide code\nfi = rf_feat_importance(m, xs)\nfi[:10]\n\n\n\n\n\n\n\n\n\n\ncols\nimp\n\n\n\n\n18\nlog_installed_capacity\n0.334581\n\n\n3\nis_consumption\n0.323513\n\n\n17\nlog_eic_count\n0.089307\n\n\n1\nis_business\n0.088815\n\n\n2\nproduct_type\n0.016892\n\n\n0\ncounty\n0.015490\n\n\n28\nlog_surface_solar_radiation_downwards_mean_fd\n0.015466\n\n\n130\ndatetimeHour\n0.011871\n\n\n78\nlog_shortwave_radiation_mean_hd\n0.011550\n\n\n14\nis_weekend\n0.009973\n\n\n\n\n\n\n\n\nWe’ll plot the scores to compare more clearly:\n\n\nHide code\nplot_fi(fi[:35]);\n\n\n\n\n\n\n\n\nFigure 3: Plotting random forest feature importances\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe feature importance algorithm calculates the scores by looping through each tree and recursing on each branch. At each branch, it looks at what feature was used for the split and how much the model improved as a result. The improvement is weighted by samples in that group and added to the importance score of that feature. These are summed across all branches of all trees. Finally, the scores for all features are normalized so they add to 1.\n\n\nlog_installed_capacity and is_consumption are the top two predictors, matching our earlier findings. Features relating to the segment, solar radiation, the time of day, and whether it’s a weekend seem intuitive as well.\nIt seems like we can simplify our model by ignoring unimportant features. We’ll try keeping the top-scoring 25%, which works out to a score of 0.001 or more.\n\n\nHide code\n# keep approx top 25-33%\nto_keep = fi[fi.imp&gt;0.001].cols\nlen(xs.columns), len(to_keep)\n\n\n(135, 31)\n\n\n\n\nHide code\nxs_imp = xs[to_keep]\nvalid_xs_imp = valid_xs[to_keep]\n\n\nAnd train again:\n\n\nHide code\n%%time\nm = rf(xs_imp, y)\n\n\nCPU times: user 11min 18s, sys: 3.05 s, total: 11min 21s\nWall time: 2min 10s\n\n\n\n\nHide code\nprint(\"train mae: \", m_mae(m, xs_imp, y))\nprint(\"valid mae: \", m_mae(m, valid_xs_imp, valid_y))\nprint(\"OOB error: \", mae(m.oob_prediction_, y))\n\n\ntrain mae:  17.5442\nvalid mae:  57.84291\nOOB error:  22.304685\n\n\nRecall our earlier model, which used all 135 features:\n\ntrain mae: 16.108207\nvalid mae: 57.444592\nOOB error: 21.931264\n\nWe’ve near matched that performance using just 31 features (less than a quarter of the total)! This is much more managable to study in depth.\nLet’s try to go further by removing potentially redundant features:\n\n\nHide code\ncluster_columns(xs_imp, figsize=(12, 11))\n\n\n\n\n\n\n\n\nFigure 4: Visualizing feature similarity\n\n\n\n\n\nfastai’s cluster_columns function calculates the similarity (rank correlation) between features. In Figure 4, the most similar features are merged early, far from the tree’s root on the left.\nThe following appear closely related since they merged earliest:\n\ndatetime, datetimeElapsed, and row_id\ntemperature_mean_fd and temperature_mean_hd\ncos(datetimeDayofyear) and sin(datetimeDayofyear)\n\nLet’s see what we else can remove without losing accuracy. We’ll quickly train a small random forest and use its OOB score attribute to measure accuracy. OOB score is R-squared, with 1.0 for a perfect model and 0.0 for a random model.\n\n\nHide code\nprint(\"max_samples: \", int(0.05*len(xs_imp)))\ndef get_oob(df):\n    m = RandomForestRegressor(n_estimators=40, min_samples_leaf=15,\n        max_samples=int(0.05*len(xs_imp)), max_features=0.5, n_jobs=-1, oob_score=True)\n    m.fit(df, y)\n    return m.oob_score_\n\n\nmax_samples:  76551\n\n\nWe’ll get a baseline score:\n\n\nHide code\n# baseline\nget_oob(xs_imp)\n\n\n0.9750720346249664\n\n\nAnd drop the potentially redundant features one at a time first:\n\n\nHide code\n%%time \n{c:get_oob(xs_imp.drop(c, axis=1)) for c in ('sin(datetimeDayofyear)', 'cos(datetimeDayofyear)', 'temperature_mean_fd', 'temperature_mean_hd', \n                                             'datetime', 'datetimeElapsed', 'row_id',)\n}\n\n\nCPU times: user 9min 24s, sys: 17.6 s, total: 9min 42s\nWall time: 3min 6s\n\n\n{'sin(datetimeDayofyear)': 0.9748840187574119,\n 'cos(datetimeDayofyear)': 0.9747854942031892,\n 'temperature_mean_fd': 0.9756331334112932,\n 'temperature_mean_hd': 0.9739451030232441,\n 'datetime': 0.9740079246614726,\n 'datetimeElapsed': 0.9750031089303453,\n 'row_id': 0.9749748373792562}\n\n\nNow let’s try removing multiple at once, one from each closely related pair.\n\n\nHide code\nto_drop = ['cos(datetimeDayofyear)', 'temperature_mean_fd', 'datetime', 'row_id']\nget_oob(xs_imp.drop(to_drop, axis=1))\n\n\n0.9750910616501632\n\n\nLooks like we can remove them with minimal impact. Let’s do so and train another model:\n\n\nHide code\nxs_final = xs_imp.drop(to_drop, axis=1)\nvalid_xs_final = valid_xs_imp.drop(to_drop, axis=1)\n\n\n\n\nHide code\n%%time\nm = rf(xs_final, y)\n\n\nCPU times: user 9min 15s, sys: 2.51 s, total: 9min 17s\nWall time: 1min 48s\n\n\n\n\nHide code\nprint(\"train mae: \", m_mae(m, xs_final, y))\nprint(\"valid mae: \", m_mae(m, valid_xs_final, valid_y))\nprint(\"OOB error: \", mae(m.oob_prediction_, y))\n\n\ntrain mae:  17.829288\nvalid mae:  49.70654\nOOB error:  22.560125\n\n\n\n\nHide code\nlen(xs.columns), len(xs_final.columns)\n\n\n(135, 27)\n\n\nBy focusing on the strongest predictors and removing redundancy, we can use just 27 features while substantially improving performance. Simpifying the model this way makes it easier to understand and maintain."
  },
  {
    "objectID": "posts/enefit-part-1/enefit-1.html#partial-dependence-analysis",
    "href": "posts/enefit-part-1/enefit-1.html#partial-dependence-analysis",
    "title": "Enefit - Predict Energy Behavior of Prosumers, part 1",
    "section": "6 Partial dependence analysis",
    "text": "6 Partial dependence analysis\nWe saw that the top two predictors were log_installed_capacity and is_consumption. Let’s examine how they vary with the target.\nIt’s worth looking at their value distributions. Here’s log_installed_capacity’s histogram:\n\n\nHide code\nvalid_xs_final['log_installed_capacity'].hist()\n\n\n\n\n\n\n\n\n\nMost of the values are between 5.5 and 8.\nSince is_consumption is categorical, we’ll get its value counts:\n\n\nHide code\nvalid_xs_final['is_consumption'].value_counts(sort=False)\n\n\n1    44904\n2    44904\nName: is_consumption, dtype: int64\n\n\nThere are as many production targets as consumption targets.\n\n\nHide code\nfrom sklearn.inspection import partial_dependence\nfrom sklearn.inspection import PartialDependenceDisplay\n\nfig,ax = plt.subplots(figsize=(12, 4))\nPartialDependenceDisplay.from_estimator(m, valid_xs_final, ['log_installed_capacity', 'is_consumption'], \n                                        grid_resolution=40, ax=ax)\n\n\n\n\n\n\n\n\nFigure 5: Partial dependence plots of the 2 strongest predictors\n\n\n\n\n\nThe plots in Figure 5 show how log_installed_capacity and is_consumption vary with the target all else being equal. This isn’t as straightforward as averaging the targets for each unique value. Instead, we copy the dataset and fix the values in log_installed_capacity to one of its represented values. We get the predictions on this made-up version and average them. This is repeated until we cover all values log_installed_capacity has taken in dataset. The plot above shows the average prediction vs log_installed_capacity’s such fixed values.\nAs we might expect, greater installed capacity correlates to higher production or consumption amounts. Between 5.5 and 8 (the bulk of the data), the target appears linearly related with log_installed_capacity, which means it’s exponentially related with the raw installed_capacity. There’s a dramatic jump after 8, likely from those infrequent but large values seen in our earlier analysis.\nThe is_consumption plot suggests that consumption amounts are significantly higher than production amounts, which matches what we saw with dtreeviz.\n\n\n\n\n\n\nImportant\n\n\n\nAs yet, there doesn’t seem to be any data leakage, where the training data includes information that wouldn’t be available at test time. We’d normally be alerted to the possibility by performance that appears too good to be true, seemingly meaningless features that turn out to be strong predictors, or partial dependence plots that don’t match our intuition."
  },
  {
    "objectID": "posts/enefit-part-1/enefit-1.html#out-of-domain-analysis",
    "href": "posts/enefit-part-1/enefit-1.html#out-of-domain-analysis",
    "title": "Enefit - Predict Energy Behavior of Prosumers, part 1",
    "section": "7 Out-of-Domain analysis",
    "text": "7 Out-of-Domain analysis\nDomain shift is expected since the data are time series, but it’s still worth looking at closer. We want to evaluate the extent of domain shift for each feature. Conveniently, we can do that with a random forest!\nFirst, we’ll re-label the data such that the model learns to predict whether a row belongs to the training or validation set and then look at its feature importances:\n\n\nHide code\n%time\ndf_dom = pd.concat([xs_final, valid_xs_final])\nis_valid = np.array([0]*len(xs_final) + [1]*len(valid_xs_final))\n\nm_dom = rf(df_dom, is_valid)\nrf_feat_importance(m_dom, df_dom)[:10]\n\n\nCPU times: user 3 µs, sys: 0 ns, total: 3 µs\nWall time: 6.91 µs\n\n\n\n\n\n\n\n\n\n\ncols\nimp\n\n\n\n\n13\ndatetimeElapsed\n0.813972\n\n\n15\ndatetimeDayofyear\n0.068870\n\n\n17\ndatetimeWeek\n0.044067\n\n\n23\nsin(datetimeDayofyear)\n0.042158\n\n\n8\nlog_shortwave_radiation_mean_hd\n0.007305\n\n\n19\nlog_diffuse_radiation_mean_hd\n0.006550\n\n\n24\ncos(datetimeHour)\n0.005165\n\n\n21\nlog_snowfall_std_fd\n0.003465\n\n\n11\ntemperature_mean_hd\n0.003244\n\n\n12\nlog_direct_solar_radiation_mean_hd\n0.002117\n\n\n\n\n\n\n\n\nAs expected, features that directly encode the date or measure weather conditions exhibit domain shift. datetimeElapsed shifts the most since it is incremented most frequently. datetimeDayofyear and datetimeWeek increment less frequently and within a narrower range of values, so it makes sense why they shift less.\nLet’s see whether removing columns with high domain shift (i.e. an importance score &gt; 0.01) helps. We’ll remove the columns one at a time and check the validation error:\n\n\nHide code\n%%time\nm = rf(xs_final, y)\nprint('baseline', m_mae(m, valid_xs_final, valid_y))\n\nfor c in ('datetimeElapsed', 'datetimeDayofyear', 'sin(datetimeDayofyear)', 'datetimeWeek'):\n    m = rf(xs_final.drop(c,axis=1), y)\n    print(c, m_mae(m, valid_xs_final.drop(c,axis=1), valid_y))\n\n\nbaseline 48.669705\ndatetimeElapsed 50.178429\ndatetimeDayofyear 47.093186\nsin(datetimeDayofyear) 46.950955\ndatetimeWeek 47.002353\nCPU times: user 46min 32s, sys: 8.86 s, total: 46min 41s\nWall time: 9min 5s\n\n\nLooks like we can drop datetimeDayofyear, sin(datetimeDayofyear), and datetimeWeek safely. Let’s see:\n\n\nHide code\n%%time\ntime_vars = ['datetimeDayofyear', 'sin(datetimeDayofyear)', 'datetimeWeek']\nxs_final_time = xs_final.drop(time_vars, axis=1)\nvalid_xs_time = valid_xs_final.drop(time_vars, axis=1)\n\nm = rf(xs_final_time, y)\nprint(\"train mae: \", m_mae(m, xs_final_time, y))\nprint(\"valid mae: \", m_mae(m, valid_xs_time, valid_y))\nprint(\"OOB error: \", mae(m.oob_prediction_, y))\n\n\ntrain mae:  18.6872\nvalid mae:  48.099852\nOOB error:  23.626528\nCPU times: user 7min 2s, sys: 1.87 s, total: 7min 3s\nWall time: 1min 23s\n\n\nLooking good. Removing columns with high domain shift will make the model more resilient.\nLastly, it may help to not use old data. Let’s see whether dropping data prior to Jan 2022 helps:\n\n\nHide code\nfilt = xs['datetimeYear']&gt;1\nxs_filt = xs_final_time[filt]\ny_filt = y[filt]\n\n\n\n\nHide code\n%%time\nm = rf(xs_filt, y_filt)\nprint(\"train mae: \", m_mae(m, xs_filt, y_filt))\nprint(\"valid mae: \", m_mae(m, valid_xs_time, valid_y))\nprint(\"OOB error: \", mae(m.oob_prediction_, y_filt))\n\n\ntrain mae:  19.664996\nvalid mae:  50.463263\nOOB error:  24.873183\nCPU times: user 5min 9s, sys: 447 ms, total: 5min 10s\nWall time: 1min 1s\n\n\n\n\nHide code\nlen(xs.columns), len(xs_filt.columns)\n\n\n(135, 24)\n\n\n\n\nHide code\nlen(xs), len(xs_filt)\n\n\n(1531020, 1188514)\n\n\nDropping the older data means we need 22% fewer records to train, but since we didn’t improve, we’ll proceed with the whole dataset.\n\n\n\n\n\n\nNote\n\n\n\nOur original random forest had a 57.4 validation error using 135 features. Our simplified model achieves a 48.1 validation error using just 18% of the features!"
  },
  {
    "objectID": "posts/enefit-part-1/enefit-1.html#tree-interpreter",
    "href": "posts/enefit-part-1/enefit-1.html#tree-interpreter",
    "title": "Enefit - Predict Energy Behavior of Prosumers, part 1",
    "section": "8 Tree interpreter",
    "text": "8 Tree interpreter\nWith the treeinterpreter and waterfallcharts libraries, we can visualize how single predictions are made.\n\n\nHide code\nfrom treeinterpreter import treeinterpreter\nfrom waterfall_chart import plot as waterfall\n\n\nLet’s grab the first few rows of the validation set and get predictions:\n\n\nHide code\nrow = valid_xs_time.iloc[:5]\n\n\n\n\nHide code\nprediction,bias,contributions = treeinterpreter.predict(m, row.values)\n\n\nJust as we looked at feature importances over the entire dataset, we can do so for a single row. Let’s check out the first row:\n\n\nHide code\n{'prediction': prediction[0], \n 'bias': bias[0],\n 'contributions_sum': contributions[0].sum()}\n\n\n{'prediction': array([4.77301322]),\n 'bias': 261.68865324823906,\n 'contributions_sum': -256.91564002804654}\n\n\n\n\nHide code\nbias[0] + contributions[0].sum()\n\n\n4.773013220192524\n\n\nThe prediction is obtained by adding the bias and the contributions’ sum. The bias is the dependent variable’s mean, i.e. the “model” at every tree’s root node. The contributions’ sum indicates the total change in the prediction, which we can visualize with a waterfall chart:\n\n\nHide code\nwaterfall(valid_xs_time.columns, contributions[0], threshold=0.175, \n          rotation_value=35,formatting='{:,.2f}');\n\n\n\n\n\n\n\n\nFigure 6: Plotting feature contributions for a single example\n\n\n\n\n\nHere’s the row with the most influential columns:\n\n\nHide code\nvalid_xs_time[:1][['is_consumption', 'log_eic_count', 'is_business', 'county']]\n\n\n\n\n\n\n\n\n\n\nis_consumption\nlog_eic_count\nis_business\ncounty\n\n\n\n\n1531020\n1\n5.961005\n1\n1\n\n\n\n\n\n\n\n\nThis row shows a production target for a non-business prosumer in Harjumaa. We can see how each feature influenced the prediction–starting from 0–with the overall change shown in net. The contributions of less influential columns are grouped together in “other”.\nWe haven’t yet considered random forests’ major limitation: their inability to extrapolate. A random forest consists of decision trees whose predictions are just the targets’ average value in a leaf node. Therefore, the trees (and hence the forest) can’t predict values outside the training data’s range! We may be able to alleviate the issue with a neural network."
  },
  {
    "objectID": "posts/enefit-part-1/enefit-1.html#modeling-with-a-neural-network",
    "href": "posts/enefit-part-1/enefit-1.html#modeling-with-a-neural-network",
    "title": "Enefit - Predict Energy Behavior of Prosumers, part 1",
    "section": "9 Modeling with a neural network",
    "text": "9 Modeling with a neural network\nWe’ll start by setting up a TabularPandas object with the trimmed feature set.\n\n\nHide code\nfrom numpy import random\nrandom.seed(42)\n\nis_partial = False\neval_month = 'feb23'\nprocs = [Categorify, Normalize]\nmax_card = 9000\n\n# need ['datetime', 'row_id'] for the splits, which are not in the final feature set. Dropped afterward. \nto_nn, final_splits = setup_training(is_partial, \n                                     features_df[list(xs_final_time.columns) + ['datetime', 'row_id', dep_var]], \n                                     pid_lookup_df, \n                                     eval_month, \n                                     procs, \n                                     max_card)\n\n\nn_pids_dropped from train: 4\npids_dropped from train: [14, 60, 20, 23]\n\n\n\n\n\n\n\n\nImportant\n\n\n\nRecall that we dropped a random set of prosumers among those common to both training and validation after the split, making the validation ones new from the model’s point of view. Re-setting the random seed ensures we drop the same set of prosumers as we did earlier.\n\n\nWhen a TabularPandas is set up, fastai has to decide which variables are continuous and which are categorical. The function cont_cat_split makes this decision based on a variable’s cardinality–how many unique values it takes–and a max_card parameter. If the cardinality exceeds max_card or the variable is a float type, it’s treated as continuous. If it’s cardinality is less than max_card, it’s categorical. Let’s check that it worked correctly:\n\n\nHide code\nto_nn.cont_names.remove('row_id')\nto_nn.cat_names.remove('datetime')\n\n{'cont_names': list(to_nn.cont_names), \n 'cat_names': list(to_nn.cat_names)\n}\n\n\n{'cont_names': ['log_installed_capacity',\n  'log_eic_count',\n  'log_surface_solar_radiation_downwards_mean_fd',\n  'log_shortwave_radiation_mean_hd',\n  'temperature_mean_hd',\n  'log_direct_solar_radiation_mean_hd',\n  'datetimeElapsed',\n  'log_surface_solar_radiation_downwards_std_fd',\n  'log_direct_solar_radiation_mean_fd',\n  'log_surface_solar_radiation_downwards_mean_fl',\n  'log_diffuse_radiation_mean_hd',\n  'log_windspeed_10m_std_hl',\n  'log_snowfall_std_fd',\n  'log_direct_solar_radiation_std_fd',\n  'cos(datetimeHour)',\n  'cloudcover_low_mean_fd',\n  'log_shortwave_radiation_mean_hl'],\n 'cat_names': ['is_consumption',\n  'is_business',\n  'product_type',\n  'county',\n  'datetimeHour',\n  'is_weekend',\n  'datetimeDayofweek']}\n\n\nLooks good. Now we’ll train the network.\n\n\nHide code\ndef nn_mae(preds, y): return torch.round((preds - y).abs().mean(), decimals=6)\n\n\nSince tabular models and datasets have a relatively low memory footprint, we can use larger batch sizes than fastai’s default (64):\n\n\nHide code\nbs = 1024\ndls = to_nn.dataloaders(bs)\n\n\n\n\nHide code\ny = to_nn.train.y\ny.min(),y.max()\n\n\n(0.0, 15480.274)\n\n\n\n\nHide code\ny_max_factor = 3\nnp.ceil(y_max_factor*y.max())\n\n\n46441.0\n\n\n\n\nHide code\ny_max = np.ceil(y_max_factor*y.max())\nlayers = [1000, 500]\n\n\n\n\nHide code\nlearn = tabular_learner(dls, y_range=(0, y_max), layers=layers,\n                        n_out=1, loss_func=F.l1_loss)\n\n\nWe know the target will always greater than or equal to 0, but a neural network can produce negative outputs. We can configure our tabular_learner’s sigmoid function with y_range to constrain its output values between 0 and some maximum. This will help stabilize training without overly compromising extrapolation flexibility. For now, I set the maximum to 3x the maximum target in the training set.\nThe default tabular_learner has 2 hidden layers, with 200 and 100 activations, respectively, which works well for small datasets. With some fiddling, I found increasing the layer sizes to 1000 and 500 activations to work reasonably well.\nTo help set a learning rate, fastai provides a handy learning rate finder:\n\n\nHide code\nlearn.lr_find(suggest_funcs=[slide, valley])\n\n\n\n\n\n\n\n\n\nSuggestedLRs(slide=6.309573450380412e-07, valley=0.001737800776027143)\n\n\n\n\n\n\n\n\n\nWe’ll use a 1e-3 learning rate and train for a few epochs:\n\n\nHide code\nlearn.fit_one_cycle(5, 1e-3)\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n137.240952\n139.941147\n00:18\n\n\n1\n68.864647\n82.996590\n00:19\n\n\n2\n53.926590\n73.568031\n00:18\n\n\n3\n45.439701\n70.161186\n00:18\n\n\n4\n41.196339\n64.239616\n00:18\n\n\n\n\n\n\nLet’s see some predictions (scroll to the right end):\n\n\nHide code\nlearn.show_results()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nis_consumption\nis_business\nproduct_type\ncounty\ndatetimeHour\nis_weekend\ndatetimeDayofweek\nlog_installed_capacity\nlog_eic_count\nlog_surface_solar_radiation_downwards_mean_fd\nlog_shortwave_radiation_mean_hd\ntemperature_mean_hd\nlog_direct_solar_radiation_mean_hd\ndatetimeElapsed\nlog_surface_solar_radiation_downwards_std_fd\nlog_direct_solar_radiation_mean_fd\nlog_surface_solar_radiation_downwards_mean_fl\nlog_diffuse_radiation_mean_hd\nlog_windspeed_10m_std_hl\nlog_snowfall_std_fd\nlog_direct_solar_radiation_std_fd\ncos(datetimeHour)\ncloudcover_low_mean_fd\nlog_shortwave_radiation_mean_hl\ntarget\ntarget_pred\n\n\n\n\n0\n2.0\n2.0\n4.0\n5.0\n18.0\n1.0\n4.0\n0.651592\n0.222225\n-0.889688\n-0.111235\n-0.893069\n-0.406757\n1.741631\n-0.816057\n-0.827848\n-0.548966\n-0.069316\n-0.374684\n0.794611\n-0.751869\n-1.365782\n1.519243\n-0.549535\n988.713989\n1015.745361\n\n\n1\n1.0\n2.0\n4.0\n12.0\n21.0\n1.0\n1.0\n2.135026\n1.542524\n-0.889688\n-0.915917\n-0.953018\n-0.774304\n1.769509\n-0.816057\n-0.827848\n-0.548966\n-0.932073\n0.081468\n-0.382855\n-0.751869\n-1.224502\n-0.978829\n-0.549535\n14.080000\n4.172789\n\n\n2\n2.0\n2.0\n2.0\n3.0\n11.0\n1.0\n2.0\n-1.226182\n-1.027644\n1.259645\n1.164032\n-0.520550\n1.319541\n1.915397\n0.696772\n1.169106\n-0.548966\n1.129292\n-0.374684\n0.514093\n1.834087\n0.707350\n0.014958\n-0.549535\n100.364998\n58.802540\n\n\n3\n1.0\n2.0\n2.0\n6.0\n15.0\n1.0\n2.0\n-0.368409\n-0.549235\n1.065332\n1.023860\n-0.763749\n0.425499\n1.774586\n0.386823\n1.212754\n-0.548966\n1.260255\n-0.374684\n-0.392570\n0.725691\n-0.706864\n0.020374\n-0.549535\n8.256000\n5.093526\n\n\n4\n1.0\n1.0\n2.0\n2.0\n8.0\n1.0\n4.0\n-1.622382\n-0.863264\n-0.402592\n-0.906819\n-1.874382\n-0.774304\n1.880760\n2.362028\n-0.511674\n-0.548966\n-0.921395\n-0.374684\n-0.257821\n0.544251\n1.366268\n0.607579\n-0.549535\n0.000000\n0.260096\n\n\n5\n1.0\n1.0\n4.0\n16.0\n21.0\n1.0\n1.0\n0.786952\n1.249527\n-0.889688\n-0.915917\n-0.953018\n-0.774304\n1.769509\n-0.816057\n-0.827848\n-0.548966\n-0.932073\n1.489535\n-0.382855\n-0.751869\n-1.224502\n-0.978829\n-0.549535\n2.062000\n2.446304\n\n\n6\n1.0\n2.0\n2.0\n8.0\n18.0\n1.0\n4.0\n0.846031\n-0.316779\n-0.350409\n0.515269\n-1.241519\n-0.750921\n1.883574\n0.927771\n-0.816394\n0.182545\n0.746684\n0.379540\n0.798027\n-0.468421\n-1.365782\n-0.063459\n1.197275\n0.006000\n2.152201\n\n\n7\n2.0\n2.0\n1.0\n8.0\n23.0\n1.0\n2.0\n0.642359\n-0.941713\n-0.889688\n-0.915917\n-0.845383\n-0.774304\n1.918782\n-0.816057\n-0.827848\n-0.548966\n-0.932073\n-0.099291\n-0.401721\n-0.751869\n-0.706864\n-1.217128\n-0.549535\n319.354004\n333.857758\n\n\n8\n2.0\n2.0\n2.0\n3.0\n3.0\n1.0\n1.0\n-1.226182\n-1.027644\n-0.889688\n-0.896151\n-0.589241\n-0.774304\n1.811756\n-0.816057\n-0.827848\n-0.548966\n-0.908876\n-0.374684\n-0.401721\n-0.751869\n0.707350\n-0.945936\n-0.549535\n69.614998\n48.941792\n\n\n\n\n\n\nThese look reasonable. Let’s train a few more epochs:\n\n\nHide code\nlearn = tabular_learner(dls, y_range=(0, y_max), layers=layers,\n                        n_out=1, loss_func=F.l1_loss)\n\nlearn.fit_one_cycle(10, 1e-3)\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\ntime\n\n\n\n\n0\n205.164886\n225.064117\n00:18\n\n\n1\n81.528191\n117.643929\n00:18\n\n\n2\n63.708591\n81.429329\n00:19\n\n\n3\n49.116337\n72.492241\n00:18\n\n\n4\n46.471172\n72.559464\n00:19\n\n\n5\n41.819759\n66.305519\n00:18\n\n\n6\n36.052628\n68.395958\n00:18\n\n\n7\n35.152107\n65.868759\n00:18\n\n\n8\n33.466084\n65.284027\n00:18\n\n\n9\n32.789875\n65.576210\n00:19\n\n\n\n\n\n\n\n\nHide code\npreds, targs = learn.get_preds()\nprint(\"nn valid mae: \", nn_mae(preds, targs))\n\n\n\n\n\n\n\n\n\nnn valid mae:  tensor(65.5762)\n\n\nThe neural net didn’t beat our random forest’s 48.1 error, but this isn’t too surprising. Unless a tabular dataset contains lots of high cardinality columns or unstructured data like natural language, neural networks don’t always offer great boosts in accuracy compared to decision tree ensembles. Neither case applies to this dataset; there’s no unstructured data, and no high cardinalities:\n\n\nHide code\nto_nn[to_nn.cat_names].nunique()\n\n\nis_consumption        2\nis_business           2\nproduct_type          4\ncounty               16\ndatetimeHour         24\nis_weekend            2\ndatetimeDayofweek     7\ndtype: int64\n\n\nLet’s see if ensembling can take us further."
  },
  {
    "objectID": "posts/enefit-part-1/enefit-1.html#ensembling",
    "href": "posts/enefit-part-1/enefit-1.html#ensembling",
    "title": "Enefit - Predict Energy Behavior of Prosumers, part 1",
    "section": "10 Ensembling",
    "text": "10 Ensembling\nEarlier, we discussed bagging as a technique to ensemble decision trees. The premise was that uncorrelated models will have uncorrelated errors, and the average of those uncorrelated errors will tend to 0. In random forests, uncorrelated errors are achieved by training the constitutent trees with random subsets of the data. However, we can get similarly uncorrelated errors by combining models that are trained using very different algorithms, as are random forests and neural networks.\nEnsembling our random forest and neural network is easy–we just average their predictions:\n\n\nHide code\nrf_preds = m.predict(valid_xs_time)\nens_preds = (to_np(preds.squeeze()) + rf_preds) /2\n\n\nLet’s check the error:\n\n\nHide code\n(ens_preds - valid_y).abs().mean()\n\n\n50.5425989624969\n\n\nNo dice this time, but we may yet improve by experimenting with the hyperparameters."
  },
  {
    "objectID": "posts/enefit-part-1/enefit-1.html#entity-embeddings-in-a-random-forest",
    "href": "posts/enefit-part-1/enefit-1.html#entity-embeddings-in-a-random-forest",
    "title": "Enefit - Predict Energy Behavior of Prosumers, part 1",
    "section": "11 Entity embeddings in a random forest",
    "text": "11 Entity embeddings in a random forest\nWe saw a tradeoff with neural networks: gains in extrapolation and potentially accuracy, but fiddly training.\nIn the paper “Entity Embeddings of Categorical Variables”, authors Guo and Berkman show how the embeddings of categorical variables can be used in a random forest. In this way, we gain much of a neural network’s performance improvement without actually using one at inference time! The process is simple: train a neural network with categorical embeddings, concatenate the learned embeddings with the continuous columns, and then train our random forest.\n\n\n\n\n\n\nNote\n\n\n\nIn entity embedding, categorical variables’ distinct values–the categories–are allotted their own set of initially random numbers (“latent factors”). An embedding is just a lookup table that maps the categories to their latent factors. So, to “embed” a category is to look up its latent factors.\nIn the data, a categorical variables’ original values are replaced with where to find their latent factors in the embedding. As we’ve seen, this is how TabularPandas encoded is_consumption. It’s original values–0 and 1–were replaced with 1 and 2, respectively. In the embedding, the latent factors for 0 and 1 are found at positions 1 and 2, respectively. This is why 1 and 2 have no particular meaning outside of this lookup operation.\nIn a neural network, an embedding is just another layer. The latent factors within it are gradually adjusted during training and eventually approach values that capture meaningful aspects of the categories. The neural network only has to be trained once; we can store the embeddings and re-use them in other models.\n\n\ntabular_learner already uses entity embedding to handle categorical variables, so we can just get its learned embeddings. The embed_features function below prepares the data:\n\n\nHide code\ndef embed_features(learner, xs):\n    # deep copy (creates new object w/ a copy of indices and data of original)\n    xs = xs.copy()\n    for i, feature in enumerate(learn.dls.cat_names):\n        \n        # get the embedding (move it to CPU first)\n        # .cpu() see nn.ModuleList.cpu?\n        # for learn.model.embeds, see learn.model??\n        emb = learn.model.embeds[i].cpu()\n        \n        # embed the categorical feature and return a df using the indices of xs\n        # for embedding_dim see nn.Embedding??\n        # join previously failed b/c col names overlap for each embedding feature\n        # they were previously all to be named feature_0, feature_1, etc.\n        new_feat = pd.DataFrame(emb(tensor(xs[feature], dtype=torch.int64)), \n                                index=xs.index, \n                                columns=[f\"{feature}_{j}\" for j in range(emb.embedding_dim)])\n        \n        # drop the categorical feature\n        xs.drop(columns=feature, inplace=True)\n        \n        # left join the embedded features w/ xs\n        # returns nn's cat embeddings concatenated w/ original cont variables\n        # the cont variables will be the same for the rf and the nn\n        xs = xs.join(new_feat)\n    return xs\n\n\n\n\nHide code\nembedded_xs = embed_features(learn, learn.dls.train.xs)\nembedded_valid_xs = embed_features(learn, learn.dls.valid.xs)\nembedded_xs.shape, embedded_valid_xs.shape\n\n\n((1531020, 53), (89808, 53))\n\n\n\n\nHide code\nembedded_xs.columns\n\n\nIndex(['log_installed_capacity', 'log_eic_count',\n       'log_surface_solar_radiation_downwards_mean_fd',\n       'log_shortwave_radiation_mean_hd', 'temperature_mean_hd',\n       'log_direct_solar_radiation_mean_hd', 'datetimeElapsed',\n       'log_surface_solar_radiation_downwards_std_fd',\n       'log_direct_solar_radiation_mean_fd',\n       'log_surface_solar_radiation_downwards_mean_fl',\n       'log_diffuse_radiation_mean_hd', 'log_windspeed_10m_std_hl',\n       'log_snowfall_std_fd', 'log_direct_solar_radiation_std_fd',\n       'cos(datetimeHour)', 'cloudcover_low_mean_fd',\n       'log_shortwave_radiation_mean_hl', 'is_consumption_0',\n       'is_consumption_1', 'is_consumption_2', 'is_business_0',\n       'is_business_1', 'is_business_2', 'product_type_0', 'product_type_1',\n       'product_type_2', 'product_type_3', 'county_0', 'county_1', 'county_2',\n       'county_3', 'county_4', 'county_5', 'county_6', 'county_7',\n       'datetimeHour_0', 'datetimeHour_1', 'datetimeHour_2', 'datetimeHour_3',\n       'datetimeHour_4', 'datetimeHour_5', 'datetimeHour_6', 'datetimeHour_7',\n       'datetimeHour_8', 'datetimeHour_9', 'is_weekend_0', 'is_weekend_1',\n       'is_weekend_2', 'datetimeDayofweek_0', 'datetimeDayofweek_1',\n       'datetimeDayofweek_2', 'datetimeDayofweek_3', 'datetimeDayofweek_4'],\n      dtype='object')\n\n\nLet’s see how we do:\n\n\nHide code\n%%time\nm_cat_emb = rf(embedded_xs, y)\n# pass m_cat_emb for the model. includes both cont variables and nn's cat embeddings\n# pass the embedded xs for both train and valid errors\nprint(\"train mae: \", m_mae(m_cat_emb, embedded_xs, y))\nprint(\"valid mae: \", m_mae(m_cat_emb, embedded_valid_xs, valid_y))\nprint(\"OOB mae: \", mae(m_cat_emb.oob_prediction_, y))\n\n\ntrain mae:  18.011094\nvalid mae:  46.230781\nOOB mae:  22.948394\nCPU times: user 10min 40s, sys: 4 s, total: 10min 44s\nWall time: 2min 10s\n\n\nNice, that’s our best result yet!"
  },
  {
    "objectID": "posts/enefit-part-1/enefit-1.html#next-steps",
    "href": "posts/enefit-part-1/enefit-1.html#next-steps",
    "title": "Enefit - Predict Energy Behavior of Prosumers, part 1",
    "section": "12 Next steps",
    "text": "12 Next steps\nIn the next post, we’ll examine production and consumption more closely and see whether modeling them separately gives better results than modeling them together. In addition, we’ll engineer lagged features and streamline the data processing pipeline.\nThanks for reading!"
  }
]